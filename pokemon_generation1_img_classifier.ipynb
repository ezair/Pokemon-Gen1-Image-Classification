{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pokemon_generation1_img_classifier",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O4AiaG4Mw9k"
      },
      "source": [
        "# Goal\r\n",
        "\r\n",
        "We are going to mutli classify the first 150 Pokemon (the entire first generation) via using image classification.\r\n",
        "\r\n",
        "## The Data\r\n",
        "\r\n",
        "We have a directory containing multiple images for each individual pokemon.\r\n",
        "\r\n",
        "\r\n",
        "## Approach\r\n",
        "\r\n",
        "In order to solve this problem we are going to construct a Convolution Neural Network.\r\n",
        "\r\n",
        "## Tools\r\n",
        "\r\n",
        "We will be using Keras for constructing our CNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL7zOtQbNNSK"
      },
      "source": [
        "## Dataset Structuring\r\n",
        "\r\n",
        "The first thing we will need to do is decide how we are going to structure our data. I have decided to start with a train test apporach for training the model, thus we will need to split the data up into test, train\r\n",
        "\r\n",
        "First, let's take a look at our data and see how many records of each pokemon we have. This will help us determine what percentage of data is going to be used for testing/training.\r\n",
        "\r\n",
        "In addition to this, we will also be able to see how many images we should use for each different Pokemon that we are classifying."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyqFe-3KOe-8"
      },
      "source": [
        "from os import listdir\r\n",
        "from os.path import join\r\n",
        "\r\n",
        "PATH_TO_PROJECT = '/content/drive/MyDrive/Colab Notebooks/Generation One Pokemon Classification/'\r\n",
        "PATH_TO_POKEMON_IMG_DIR = '/content/drive/MyDrive/Colab Notebooks/Generation One Pokemon Classification/pokemon.zip (Unzipped Files)/dataset/'\r\n",
        "\r\n",
        "\r\n",
        "def get_dict_of_image_count_for_each_pokemon(path_to_dir_containing_pokemon_img_dirs):\r\n",
        "    dict_of_pokemon_image_counts = {}\r\n",
        "    inner_pokemon_dirs = listdir(path_to_dir_containing_pokemon_img_dirs)\r\n",
        "    \r\n",
        "    for pokemon_dir_name in inner_pokemon_dirs:\r\n",
        "        # It's nice to keep records in alphabetical order.\r\n",
        "        path_to_pokemon_images = join(path_to_dir_containing_pokemon_img_dirs, pokemon_dir_name)\r\n",
        "        if '.' not in pokemon_dir_name:\r\n",
        "            dict_of_pokemon_image_counts[pokemon_dir_name] = len(listdir(path_to_pokemon_images))\r\n",
        "    \r\n",
        "    return dict_of_pokemon_image_counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXzfnkw1SfUz",
        "outputId": "ad3c4b27-c0b1-4058-8d52-faa0c7a8919f"
      },
      "source": [
        "\r\n",
        "pokemon_count_dict = get_dict_of_image_count_for_each_pokemon(PATH_TO_POKEMON_IMG_DIR)\r\n",
        "\r\n",
        "for pokemon in pokemon_count_dict:\r\n",
        "    print(f\"{pokemon}: {pokemon_count_dict[pokemon]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Alakazam: 49\n",
            "Abra: 42\n",
            "Arbok: 63\n",
            "Aerodactyl: 97\n",
            "Arcanine: 61\n",
            "Articuno: 56\n",
            "Beedrill: 53\n",
            "Bellsprout: 55\n",
            "Blastoise: 62\n",
            "Bulbasaur: 289\n",
            "Butterfree: 66\n",
            "Caterpie: 50\n",
            "Chansey: 58\n",
            "Charizard: 52\n",
            "Charmander: 296\n",
            "Charmeleon: 65\n",
            "Clefable: 49\n",
            "Clefairy: 60\n",
            "Cloyster: 60\n",
            "Cubone: 59\n",
            "Dewgong: 67\n",
            "Diglett: 51\n",
            "Ditto: 49\n",
            "Dodrio: 65\n",
            "Doduo: 48\n",
            "Dragonair: 65\n",
            "Dragonite: 62\n",
            "Dratini: 109\n",
            "Drowzee: 60\n",
            "Dugtrio: 64\n",
            "Eevee: 41\n",
            "Ekans: 52\n",
            "Electrode: 67\n",
            "Electabuzz: 54\n",
            "Exeggcute: 57\n",
            "Exeggutor: 70\n",
            "Farfetchd: 64\n",
            "Fearow: 124\n",
            "Flareon: 59\n",
            "Gastly: 50\n",
            "Gengar: 60\n",
            "Geodude: 56\n",
            "Golbat: 67\n",
            "Gloom: 58\n",
            "Goldeen: 58\n",
            "Golduck: 61\n",
            "Golem: 64\n",
            "Graveler: 58\n",
            "Grimer: 64\n",
            "Growlithe: 69\n",
            "Gyarados: 68\n",
            "Haunter: 63\n",
            "Hitmonchan: 61\n",
            "Hitmonlee: 65\n",
            "Horsea: 63\n",
            "Hypno: 63\n",
            "Ivysaur: 53\n",
            "Jigglypuff: 65\n",
            "Jolteon: 64\n",
            "Jynx: 59\n",
            "Kabuto: 56\n",
            "Kabutops: 66\n",
            "Kadabra: 61\n",
            "Kakuna: 67\n",
            "Kangaskhan: 63\n",
            "Kingler: 69\n",
            "Koffing: 65\n",
            "Krabby: 64\n",
            "Lapras: 71\n",
            "Lickitung: 67\n",
            "Machamp: 72\n",
            "Machoke: 51\n",
            "Machop: 53\n",
            "Magikarp: 59\n",
            "Magmar: 59\n",
            "Magnemite: 60\n",
            "Magneton: 59\n",
            "Mankey: 72\n",
            "Marowak: 70\n",
            "Meowth: 70\n",
            "Metapod: 65\n",
            "Mew: 67\n",
            "Mewtwo: 307\n",
            "Moltres: 62\n",
            "MrMime: 59\n",
            "Muk: 71\n",
            "Nidoking: 68\n",
            "Nidoqueen: 66\n",
            "Nidorina: 59\n",
            "Nidorino: 63\n",
            "Ninetales: 74\n",
            "Oddish: 66\n",
            "Omanyte: 55\n",
            "Omastar: 58\n",
            "Onix: 64\n",
            "Paras: 55\n",
            "Parasect: 53\n",
            "Persian: 57\n",
            "Pidgeot: 66\n",
            "Pidgeotto: 63\n",
            "Pidgey: 74\n",
            "Pikachu: 298\n",
            "Pinsir: 63\n",
            "Poliwag: 65\n",
            "Poliwhirl: 64\n",
            "Poliwrath: 63\n",
            "Ponyta: 66\n",
            "Porygon: 55\n",
            "Primeape: 70\n",
            "Psyduck: 159\n",
            "Raichu: 71\n",
            "Rapidash: 83\n",
            "Raticate: 70\n",
            "Rattata: 67\n",
            "Rhydon: 59\n",
            "Rhyhorn: 69\n",
            "Sandshrew: 66\n",
            "Sandslash: 71\n",
            "Scyther: 68\n",
            "Seadra: 67\n",
            "Seaking: 65\n",
            "Seel: 61\n",
            "Shellder: 81\n",
            "Slowbro: 63\n",
            "Slowpoke: 60\n",
            "Snorlax: 69\n",
            "Spearow: 130\n",
            "Squirtle: 280\n",
            "Starmie: 59\n",
            "Staryu: 60\n",
            "Tangela: 63\n",
            "Tauros: 67\n",
            "Tentacool: 57\n",
            "Tentacruel: 57\n",
            "Vaporeon: 68\n",
            "Venomoth: 66\n",
            "Venonat: 57\n",
            "Venusaur: 66\n",
            "Victreebel: 55\n",
            "Vileplume: 67\n",
            "Voltorb: 65\n",
            "Vulpix: 68\n",
            "Wartortle: 61\n",
            "Weepinbell: 51\n",
            "Weedle: 59\n",
            "Weezing: 63\n",
            "Wigglytuff: 67\n",
            "Zubat: 49\n",
            "Zapdos: 60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZU-7Ex8WaiA"
      },
      "source": [
        "## Constructing a Fair Dataset\r\n",
        "Looking at the number of Pokemon images associated with each distinct Pokemon, we notice that they are all not the same. Some Pokemon have more images in the dataset than others.\r\n",
        "\r\n",
        "In order to keepa certain level of fairness/distribution for the training of our future CNN, we are going to take a look at the pokemon with the least amount of images, `N`. We will then take `N` number of images per each Pokemon. These images will then be added to one large dataset that we will later split into a training dataset and a testing dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npgNb4cOY97g"
      },
      "source": [
        "import statistics\r\n",
        "\r\n",
        "# Let's find out which Pokemon has least number of images and how few images that is.\r\n",
        "pokemon_with_the_least_imgs = min(pokemon_count_dict.keys(), key=lambda k: pokemon_count_dict[k])\r\n",
        "least_number_of_imgs = pokemon_count_dict[pokemon_with_the_least_imgs]\r\n",
        "\r\n",
        "pokemon_with_the_most_imgs = max(pokemon_count_dict.keys(), key=lambda k: pokemon_count_dict[k])\r\n",
        "most_number_of_imgs = pokemon_count_dict[pokemon_with_the_most_imgs]\r\n",
        "\r\n",
        "mean_number_of_imgs = sum([pokemon_count_dict[pokemon] for pokemon in pokemon_count_dict]) / len(pokemon_count_dict)\r\n",
        "\r\n",
        "median_number_of_imgs = statistics.median([pokemon_count_dict[pokemon] for pokemon in pokemon_count_dict])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEL8YHXRZ8Hm",
        "outputId": "eebf85c3-8fa3-429f-a1b2-a30619f58dd5"
      },
      "source": [
        "print(f'Pokemon with the least number of images: {pokemon_with_the_least_imgs}')\r\n",
        "print(f'Least number of images: {least_number_of_imgs}\\n')\r\n",
        "\r\n",
        "print(f'Pokemon with the most number of images: {pokemon_with_the_most_imgs}')\r\n",
        "print(f'Most number of images: {most_number_of_imgs}\\n')\r\n",
        "\r\n",
        "print(f'Range number of images: {most_number_of_imgs - least_number_of_imgs}\\n')\r\n",
        "\r\n",
        "print(f'Mean average of images : {mean_number_of_imgs}\\n')\r\n",
        "\r\n",
        "print(f'Median average of images: {median_number_of_imgs}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pokemon with the least number of images: Eevee\n",
            "Least number of images: 41\n",
            "\n",
            "Pokemon with the most number of images: Mewtwo\n",
            "Most number of images: 307\n",
            "\n",
            "Range number of images: 266\n",
            "\n",
            "Mean average of images : 71.76510067114094\n",
            "\n",
            "Median average of images: 63\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVsGzwCJaPV2"
      },
      "source": [
        "Now that we know that the least amount of images for a pokemon out of all the data, let's construct a data set with 41 **random** images of each pokemon.\r\n",
        "\r\n",
        "Note we take **random** images from each Pokemon to avoid fitting the data in a specific way. In general, randomizing data is always the best option, as it helps to avoid fitting the model on a speicfic sort of pattern. Any time we can randomize inputs, we should do it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwJiMxXAbTou"
      },
      "source": [
        "*NOTE*: For the sake of clarity when constructing this model, we will use a specific random seed. This will avoid us receiving completely different model accuracies on every run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqyDSp1SbyK5"
      },
      "source": [
        "import numpy as np\r\n",
        "import random as python_random\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "# This is commonly used as a random seed for things. Ultimately it does not really matter what we choose however.\r\n",
        "# We simply use 42 for consistentcy.\r\n",
        "random_seed = 42\r\n",
        "\r\n",
        "# Okay, now we will not have in consistant data splitate moving forward,\r\n",
        "# but the images we split will still be randomly picked.\r\n",
        "# Set random seeds.\r\n",
        "np.random.seed(random_seed) \r\n",
        "python_random.seed(random_seed)\r\n",
        "tf.random.set_seed(random_seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zRWywZ0ckqk",
        "outputId": "1b9ab9aa-6379-49ae-e9fc-3b939d3635ec"
      },
      "source": [
        "# Now we will construct the overall Pokemon dataset that we will be using.\r\n",
        "# We will be storing this dataset in the location \"<path to project location>/Sample-Dataset\"\r\n",
        "path_to_dataset = join(PATH_TO_PROJECT, 'Sample-Dataset')\r\n",
        "print(\"Path to dataset:\", path_to_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Path to dataset: /content/drive/MyDrive/Colab Notebooks/Generation One Pokemon Classification/Sample-Dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DC6bzYXeKY6"
      },
      "source": [
        "from os import mkdir\r\n",
        "from shutil import rmtree\r\n",
        "from os.path import exists\r\n",
        "\r\n",
        "# Don't wanna re-create the dataset, it takes a long time...\r\n",
        "if not exists(path_to_dataset):\r\n",
        "    mkdir(path_to_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8wsUl0hgLne"
      },
      "source": [
        "Alright, finally, let's populate our new dir with images that we will feed to our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-A0rmf3gUR_"
      },
      "source": [
        "from shutil import copyfile\r\n",
        "\r\n",
        "def construct_dataset_of_random_images(path_to_dataset_dir, number_of_random_images, random_seed=None):\r\n",
        "    all_random_images_to_add_to_dataset = []\r\n",
        "    image_src_path_mapped_to_destination_path = {}\r\n",
        "\r\n",
        "    for pokemon_dir_name in listdir(PATH_TO_POKEMON_IMG_DIR):\r\n",
        "        path_to_individual_pokemon_img_dir = join(PATH_TO_POKEMON_IMG_DIR, pokemon_dir_name)\r\n",
        "        pokemon_images = listdir(path_to_individual_pokemon_img_dir)\r\n",
        "\r\n",
        "        # Need to make sure we get the absolute path of each image, as we will need it later when\r\n",
        "        # copying images to another dir.\r\n",
        "        for i in range(len(pokemon_images)):\r\n",
        "            # Need to make sure we have the full abs path to each Pokemon image.\r\n",
        "            pokemon_images[i] = join(path_to_individual_pokemon_img_dir, pokemon_images[i])\r\n",
        "        \r\n",
        "        # Ensure that we mix up the images in a random order.\r\n",
        "        np.random.shuffle(pokemon_images)\r\n",
        "\r\n",
        "        # Add them to the dictinoary so we can map there src path with there destination path\r\n",
        "        # as well as tag each image with the type of pokemon they are and index they are.\r\n",
        "        # This will make splitting our training and test data a lot easier and fair.\r\n",
        "        for i in range(number_of_random_images):\r\n",
        "\r\n",
        "            # Need to actually create the folder that all of this Pokemon's images will live in\r\n",
        "            # but must make sure we do not create the folder on every iteration.\r\n",
        "            destination_folder_path = join(path_to_dataset_dir, pokemon_dir_name)\r\n",
        "            if not exists(destination_folder_path):\r\n",
        "                mkdir(destination_folder_path)\r\n",
        "\r\n",
        "            # Populate the dict with the current path of the pokemon -> location that it will be moved to.\r\n",
        "            # Note: In the destination we need to remember to add the image .ext or things will get really bad.\r\n",
        "            img_ext = pokemon_images[i].split('.')[-1]\r\n",
        "            image_src_path_mapped_to_destination_path[pokemon_images[i]] = join(destination_folder_path,\r\n",
        "                                                                                f\"{pokemon_dir_name}_{i}.{img_ext}\")\r\n",
        "\r\n",
        "    # Copy images to the location of our dataset folder.\r\n",
        "    count_of_images = 0\r\n",
        "    for img_src_path in image_src_path_mapped_to_destination_path:\r\n",
        "        img_destination_path = image_src_path_mapped_to_destination_path[img_src_path]\r\n",
        "        \r\n",
        "        # This is so we can keep track of the state we are at ourselves.\r\n",
        "        count_of_images += 1\r\n",
        "        print(f\"Copying image number {count_of_images} out of {number_of_random_images * 150} total\")\r\n",
        "        print(f\"Image Destination is name: {img_destination_path}\\n\")\r\n",
        "\r\n",
        "        if not exists(img_destination_path):\r\n",
        "            copyfile(img_src_path, img_destination_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERkeIlq6nkQI"
      },
      "source": [
        "# NOTE: This might take a while if it is the first time you are running this line.\r\n",
        "\r\n",
        "# We will only construct the dataset if it has not yet been constructed. If there is an image in the folder, then we know it was already made.\r\n",
        "if not listdir(path_to_dataset):\r\n",
        "    construct_dataset_of_random_images(path_to_dataset_dir=path_to_dataset, number_of_random_images=least_number_of_imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYDCs-OK3azT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4522790e-991b-46a6-eb1a-2d8b60e52468"
      },
      "source": [
        "# Let's just confirm we have an even amount of images for each Pokemon.\r\n",
        "path_to_sample_dataset = '/content/drive/MyDrive/Colab Notebooks/Generation One Pokemon Classification/Sample-Dataset'\r\n",
        "pokemon_count_dict = get_dict_of_image_count_for_each_pokemon(path_to_sample_dataset)\r\n",
        "for pokemon in pokemon_count_dict:\r\n",
        "    print(f\"{pokemon}: {pokemon_count_dict[pokemon]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Alakazam: 41\n",
            "Abra: 41\n",
            "Arbok: 41\n",
            "Aerodactyl: 41\n",
            "Arcanine: 41\n",
            "Articuno: 41\n",
            "Beedrill: 41\n",
            "Bellsprout: 41\n",
            "Blastoise: 41\n",
            "Bulbasaur: 41\n",
            "Butterfree: 41\n",
            "Caterpie: 41\n",
            "Chansey: 41\n",
            "Charizard: 41\n",
            "Charmander: 41\n",
            "Charmeleon: 41\n",
            "Clefable: 41\n",
            "Clefairy: 41\n",
            "Cloyster: 41\n",
            "Cubone: 41\n",
            "Dewgong: 41\n",
            "Diglett: 41\n",
            "Ditto: 41\n",
            "Dodrio: 41\n",
            "Doduo: 41\n",
            "Dragonair: 41\n",
            "Dragonite: 41\n",
            "Dratini: 41\n",
            "Drowzee: 41\n",
            "Dugtrio: 41\n",
            "Eevee: 41\n",
            "Ekans: 41\n",
            "Electrode: 41\n",
            "Electabuzz: 41\n",
            "Exeggcute: 41\n",
            "Exeggutor: 41\n",
            "Farfetchd: 41\n",
            "Fearow: 41\n",
            "Flareon: 41\n",
            "Gastly: 41\n",
            "Gengar: 41\n",
            "Geodude: 41\n",
            "Golbat: 41\n",
            "Gloom: 41\n",
            "Goldeen: 41\n",
            "Golduck: 41\n",
            "Golem: 41\n",
            "Graveler: 41\n",
            "Grimer: 41\n",
            "Growlithe: 41\n",
            "Gyarados: 41\n",
            "Haunter: 41\n",
            "Hitmonchan: 41\n",
            "Hitmonlee: 41\n",
            "Horsea: 41\n",
            "Hypno: 41\n",
            "Ivysaur: 41\n",
            "Jigglypuff: 41\n",
            "Jolteon: 41\n",
            "Jynx: 41\n",
            "Kabuto: 41\n",
            "Kabutops: 41\n",
            "Kadabra: 41\n",
            "Kakuna: 41\n",
            "Kangaskhan: 41\n",
            "Kingler: 41\n",
            "Koffing: 41\n",
            "Krabby: 41\n",
            "Lapras: 41\n",
            "Lickitung: 41\n",
            "Machamp: 41\n",
            "Machoke: 41\n",
            "Machop: 41\n",
            "Magikarp: 41\n",
            "Magmar: 41\n",
            "Magnemite: 41\n",
            "Magneton: 41\n",
            "Mankey: 41\n",
            "Marowak: 41\n",
            "Meowth: 41\n",
            "Metapod: 41\n",
            "Mew: 41\n",
            "Mewtwo: 41\n",
            "Moltres: 41\n",
            "MrMime: 41\n",
            "Muk: 41\n",
            "Nidoking: 41\n",
            "Nidoqueen: 41\n",
            "Nidorina: 41\n",
            "Nidorino: 41\n",
            "Ninetales: 41\n",
            "Oddish: 41\n",
            "Omanyte: 41\n",
            "Omastar: 41\n",
            "Onix: 41\n",
            "Paras: 41\n",
            "Parasect: 41\n",
            "Persian: 41\n",
            "Pidgeot: 41\n",
            "Pidgeotto: 41\n",
            "Pidgey: 41\n",
            "Pikachu: 41\n",
            "Pinsir: 41\n",
            "Poliwag: 41\n",
            "Poliwhirl: 41\n",
            "Poliwrath: 41\n",
            "Ponyta: 41\n",
            "Porygon: 41\n",
            "Primeape: 41\n",
            "Psyduck: 41\n",
            "Raichu: 41\n",
            "Rapidash: 41\n",
            "Raticate: 41\n",
            "Rattata: 41\n",
            "Rhydon: 41\n",
            "Rhyhorn: 41\n",
            "Sandshrew: 41\n",
            "Sandslash: 41\n",
            "Scyther: 41\n",
            "Seadra: 41\n",
            "Seaking: 41\n",
            "Seel: 41\n",
            "Shellder: 41\n",
            "Slowbro: 41\n",
            "Slowpoke: 41\n",
            "Snorlax: 41\n",
            "Spearow: 41\n",
            "Squirtle: 41\n",
            "Starmie: 41\n",
            "Staryu: 41\n",
            "Tangela: 41\n",
            "Tauros: 41\n",
            "Tentacool: 41\n",
            "Tentacruel: 41\n",
            "Vaporeon: 41\n",
            "Venomoth: 41\n",
            "Venonat: 41\n",
            "Venusaur: 41\n",
            "Victreebel: 41\n",
            "Vileplume: 41\n",
            "Voltorb: 41\n",
            "Vulpix: 41\n",
            "Wartortle: 41\n",
            "Weepinbell: 41\n",
            "Weedle: 41\n",
            "Weezing: 41\n",
            "Wigglytuff: 41\n",
            "Zubat: 41\n",
            "Zapdos: 41\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-m9UPG1KDQU"
      },
      "source": [
        "## Split Dataset Into Test/Train\r\n",
        "\r\n",
        "Alright, now that we have our correct dataset, containing the first 150 pokemon and K images of all of them, it's time to split our data set up.\r\n",
        "\r\n",
        "For this I think that a Training set of 80% and a Testing set of 20% is a good split.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibp2-04O-q0v"
      },
      "source": [
        "### Fair Split\r\n",
        "\r\n",
        "We want to make sure to do a fair random split of the data.\r\n",
        "\r\n",
        "As in we want to split 20 percent of each different class up into our dataset. This will ensure that we have a good amount of records for every different Pokemon in our dataset.\r\n",
        "\r\n",
        "Since we have tagged image with the name of the Pokemon followed by the number image it is of that Pokemon, we can much easier ensure that this data is split fairly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuCowO6T-p9k"
      },
      "source": [
        "from keras_preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImdWSOL25PqV"
      },
      "source": [
        "# Let's set a batch size and epochs right now.\r\n",
        "# We will use these for all of the models that we create.\r\n",
        "batch_size = 32\r\n",
        "epochs = 150"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbGB9qfnCrl-"
      },
      "source": [
        "train_dir = '/content/drive/MyDrive/Colab Notebooks/Generation One Pokemon Classification/train'\r\n",
        "test_dir = '/content/drive/MyDrive/Colab Notebooks/Generation One Pokemon Classification/test'\r\n",
        "\r\n",
        "def populate_test_and_train_dirs(path_to_dataset, train_dir, test_dir, test_size=0.2):\r\n",
        "    for pokemon_dir in listdir(path_to_dataset):\r\n",
        "        full_path_to_pokemon_dir = join(path_to_dataset, pokemon_dir)\r\n",
        "        \r\n",
        "        number_of_imgs_for_each_pokemon = len(listdir(full_path_to_pokemon_dir))\r\n",
        "        print(number_of_imgs_for_each_pokemon)\r\n",
        "\r\n",
        "        dir_for_training_pokemon_imgs = join(train_dir, pokemon_dir)\r\n",
        "        dir_for_testing_pokemon_imgs = join(test_dir, pokemon_dir)\r\n",
        "\r\n",
        "        if not exists(dir_for_testing_pokemon_imgs):\r\n",
        "            mkdir(dir_for_testing_pokemon_imgs)\r\n",
        "\r\n",
        "        if not exists(dir_for_training_pokemon_imgs):\r\n",
        "            mkdir(dir_for_training_pokemon_imgs)\r\n",
        "\r\n",
        "        # Keep count of how many images we need for testing and for training.\r\n",
        "        # This will vary based on what our test_size is.\r\n",
        "        number_of_imgs_used_for_testing = round(number_of_imgs_for_each_pokemon * test_size)\r\n",
        "        number_of_imgs_used_for_training = number_of_imgs_for_each_pokemon - number_of_imgs_used_for_testing\r\n",
        "\r\n",
        "        # Since the images are already shuffeled in random order, we can just add all the needed training imgs\r\n",
        "        # and then add all the needed testing images.\r\n",
        "        for pokemon_img in listdir(full_path_to_pokemon_dir):\r\n",
        "            print(pokemon_img)\r\n",
        "            path_to_pokemon_img = join(full_path_to_pokemon_dir, pokemon_img)\r\n",
        "\r\n",
        "            if len(listdir(join(test_dir, pokemon_dir))) != number_of_imgs_used_for_testing:\r\n",
        "                destination_path_for_img = join(test_dir, pokemon_dir, pokemon_img)\r\n",
        "            else:\r\n",
        "                destination_path_for_img = join(train_dir, pokemon_dir, pokemon_img)\r\n",
        "\r\n",
        "            copyfile(path_to_pokemon_img, destination_path_for_img)\r\n",
        "\r\n",
        "\r\n",
        "            print(f\"Src: {path_to_pokemon_img}\")\r\n",
        "            print(f\"Destination: {destination_path_for_img}\\n\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjTmE9t2ETza"
      },
      "source": [
        "\r\n",
        "# Create folders to store training and testing data. Now let's randomly split them...\r\n",
        "if not exists(train_dir):\r\n",
        "    mkdir(train_dir)\r\n",
        "\r\n",
        "if not exists(test_dir):\r\n",
        "    mkdir(test_dir)\r\n",
        "    populate_test_and_train_dirs(path_to_dataset=path_to_sample_dataset, train_dir=train_dir, test_dir=test_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGT1nWmeXaDG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38fcd4f9-2709-4083-df71-d0a212a561b4"
      },
      "source": [
        "# Okay, now we can load our split up data into directly into keras.\r\n",
        "datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True,\r\n",
        "                             validation_split=0.2)\r\n",
        "\r\n",
        "train_generator = datagen.flow_from_directory(train_dir, batch_size=batch_size, subset='training',\r\n",
        "                                              seed=random_seed)\r\n",
        "\r\n",
        "test_generator = datagen.flow_from_directory(test_dir, batch_size=batch_size, subset='validation',\r\n",
        "                                             seed=random_seed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 4001 images belonging to 149 classes.\n",
            "Found 149 images belonging to 149 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw66X9o1vzkx"
      },
      "source": [
        "## Build Model\r\n",
        "\r\n",
        "Now that we have our dataset constructed and our data is loaded into Keras, it is time to construct or convolutional Neural Network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCuuYNOq2jxX"
      },
      "source": [
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\r\n",
        "from keras.layers import Conv2D, MaxPooling2D\r\n",
        "from keras import regularizers, optimizers, Sequential\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVWhJOAJwT_9"
      },
      "source": [
        "def build_model(number_of_hidden_layers=3,\r\n",
        "                hidden_activation_function='relu',\r\n",
        "                output_activation_function='softmax',\r\n",
        "                dropout=None,\r\n",
        "                input_shape=(244, 244, 3),\r\n",
        "                pool_size=(2, 2)\r\n",
        "                ):\r\n",
        "    model = Sequential()\r\n",
        "\r\n",
        "    # Let's do the input and first hidden layer seperately.\r\n",
        "    model.add(Conv2D(32, 3, 3, input_shape=input_shape))\r\n",
        "    model.add(Activation(hidden_activation_function))\r\n",
        "    model.add(MaxPooling2D(pool_size=pool_size))\r\n",
        "\r\n",
        "    # Add all other hidden layers.\r\n",
        "    for i in range(number_of_hidden_layers - 1):\r\n",
        "        model.add(Conv2D(32, 3, 3, input_shape=input_shape))\r\n",
        "        model.add(Activation(hidden_activation_function))\r\n",
        "        model.add(MaxPooling2D(pool_size=pool_size))\r\n",
        "\r\n",
        "    # Final output layer...\r\n",
        "    model.add(Flatten())\r\n",
        "    model.add(Dense(64))\r\n",
        "    model.add(Activation(hidden_activation_function))\r\n",
        "\r\n",
        "    # Apply drouput last layer.\r\n",
        "    if dropout:\r\n",
        "        model.add(Dropout(dropout))\r\n",
        "\r\n",
        "    # 150 for the 150 classes of pokemon that we have.\r\n",
        "    model.add(Dense(150))\r\n",
        "    model.add(Activation(output_activation_function))\r\n",
        "\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSWS7CvD5EUB"
      },
      "source": [
        "We will assign a dropout of 0.5 for this model, as our first test.\r\n",
        "\r\n",
        "This model will only have 3 hidden layers.\r\n",
        "\r\n",
        "We will start simple, diagnose our problems, then produce better more effective models.\r\n",
        "\r\n",
        "For an optimizer, we will be using adam. It is alway a good one to start with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eDzMfVo1Tdw"
      },
      "source": [
        "model_1 = build_model(dropout=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUjCHdmA4i4F"
      },
      "source": [
        "For our loss we will use Sparse Categorical Crossentropy.\r\n",
        "\r\n",
        "Our minimization/optimizaiton function will be adam, as it performs well in almost all conditions. This also optimizes each tuning paramater as the model's acceleration changes, allowing for faster convergence and less epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snR2grP85JiR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "46d5e35f-5f7b-4d75-cbcf-c55559395e7f"
      },
      "source": [
        "model_1.compile(loss='categorical_crossentropy',\r\n",
        "                optimizer='adam',\r\n",
        "                metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e8a3b5d0af79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model_1.compile(loss='categorical_crossentropy',\n\u001b[0m\u001b[1;32m      2\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                 metrics=['accuracy'])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model_1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxewXPfC56ur"
      },
      "source": [
        "Let's take a quick look at this model's specs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9UbuCFP55OH",
        "outputId": "bb349f8c-e168-46a5-e4da-27ebc8c44653"
      },
      "source": [
        "model_1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_47 (Conv2D)           (None, 81, 81, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_67 (Activation)   (None, 81, 81, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_44 (MaxPooling (None, 40, 40, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_48 (Conv2D)           (None, 13, 13, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_68 (Activation)   (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_45 (MaxPooling (None, 6, 6, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_49 (Conv2D)           (None, 2, 2, 32)          9248      \n",
            "_________________________________________________________________\n",
            "activation_69 (Activation)   (None, 2, 2, 32)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_46 (MaxPooling (None, 1, 1, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_12 (Flatten)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 64)                2112      \n",
            "_________________________________________________________________\n",
            "activation_70 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 150)               9750      \n",
            "_________________________________________________________________\n",
            "activation_71 (Activation)   (None, 150)               0         \n",
            "=================================================================\n",
            "Total params: 31,254\n",
            "Trainable params: 31,254\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHskFvAr6Iwt"
      },
      "source": [
        "Now, for fitting the model, we will start by running a large number of epochs and work our way down later, if we can receive a better loss value at a lower epoch.\r\n",
        "\r\n",
        "We will use a small null batchsize for this larger epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "tmhVuObf6HbX",
        "outputId": "b5bac6e0-5128-433c-90fa-3888327b8cbc"
      },
      "source": [
        "model_1.fit(train_generator, epochs=150, batch_size=15)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-135-4efe9d5062f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m:  logits and labels must be broadcastable: logits_size=[32,150] labels_size=[32,149]\n\t [[node categorical_crossentropy/softmax_cross_entropy_with_logits (defined at <ipython-input-135-4efe9d5062f0>:1) ]] [Op:__inference_train_function_10476]\n\nFunction call stack:\ntrain_function\n"
          ]
        }
      ]
    }
  ]
}